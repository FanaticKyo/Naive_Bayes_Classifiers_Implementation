2.1 The problem of testing on a test set with only 18 speeches can result in a large variance on our overall accuracy, precision, and recall. For better evaluation, we can use the N-fold cross validation to address this issue. We can take the average of the accuracies, precisions, and recalls through N iterations. The drawback is that it will definitely cost much more time to perform cross validation than conventional data split.
2.2 The third, fifth, and thirteenth sentence in the test data are more certain to be blue than others. Yes, the probabilities for those wrong classifications show relatively low confidence. For instance, the fourth sentence in the test data, which is classified as red but the actual label is blue, has very close log probability between two classes ({'red': -8999.318709189913, 'blue': -9005.02093246553}).
2.3 Yes, the classification accuracy increased from 0.944 to 1.0 on train.txt and test.txt. The accuracy also increased from 0.777 to 0.944 on train2.txt and test2.txt. Instead of using the single word as its feature, I choose to use bigram to generate the word's and its context as features. If we cannot find the bigram in test set using the model trained on the training set, then we will choose to use the unigram probability of the first word of the bigram but with a significant smaller weight. By doing this, the Naive Bayes probability will be mainly impacted by the bigram probabilities and we use unigram probabilities to calibrate the overall sentence probability.